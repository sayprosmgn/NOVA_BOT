import schedule
import time
import feedparser
import pickle
import os
import openai
import requests
from config import OPENAI_KEY, GOOGLE_KEY, YANDEX_KEY, YANDEX_FOLDER_ID
from handlers.reactions import get_reactions_keyboard
import hashlib

POSTED_LINKS_FILE = "posted_links.pkl"

def short_id(link):
    return hashlib.md5(link.encode()).hexdigest()[:16]

def load_posted_links():
    if os.path.exists(POSTED_LINKS_FILE):
        with open(POSTED_LINKS_FILE, "rb") as f:
            return pickle.load(f)
    return set()

def save_posted_links(posted_links):
    with open(POSTED_LINKS_FILE, "wb") as f:
        pickle.dump(posted_links, f)

posted_links = load_posted_links()

# RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–æ—Å—Ç–∞–≤—å –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–∏)
RSS_FEEDS = [
    "http://feeds.bbci.co.uk/news/world/rss.xml",
    "https://feeds.nbcnews.com/nbcnews/public/news",
    # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ ...
    "https://www.aljazeera.com/xml/rss/all.xml",
    "https://www.reutersagency.com/feed/?best-topics=world&post_type=best",
    "https://apnews.com/rss",
]

COUNTRY_TOPICS = {
    '–°–®–ê':      ['usa', 'trump', 'biden', 'white house', 'us ', 'washington', 'america', 'cnn', 'nbc', 'new york'],
    '–í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è': ['uk', 'britain', 'london', 'bbc', 'sky news', 'guardian'],
    '–ò–∑—Ä–∞–∏–ª—å':  ['israel', 'jerusalem', 'tel aviv', 'idf', 'gaza', 'hamas', 'jpost', 'haaretz'],
    '–†–æ—Å—Å–∏—è':   ['russia', 'moscow', 'kremlin', 'putin', 'rbc', 'lenta', 'tass', 'meduza'],
    '–ö–∏—Ç–∞–π':    ['china', 'beijing', 'xi jinping', 'scmp'],
    '–Ø–ø–æ–Ω–∏—è':   ['japan', 'tokyo', 'kyodo'],
    '–ò–Ω–¥–∏—è':    ['india', 'delhi', 'hindustan'],
    '–ì–µ—Ä–º–∞–Ω–∏—è': ['germany', 'berlin', 'dw', 'spiegel'],
    '–§—Ä–∞–Ω—Ü–∏—è':  ['france', 'paris', 'france 24'],
    '–ë–ª–∏–∂–Ω–∏–π –í–æ—Å—Ç–æ–∫': ['iran', 'iraq', 'syria', 'qatar', 'arab', 'al arabiya', 'aljazeera'],
}
EVENT_TOPICS = {
    '–ú—É–∑—ã–∫–∞': ['music', 'concert', 'song', 'album', 'duet', 'popstar', '–º—É–∑—ã–∫–∞', '–ø–µ—Å–Ω—è', '–∫–æ–Ω—Ü–µ—Ä—Ç'],
    '–ü–æ–ª–∏—Ç–∏–∫–∞': ['politics', 'president', 'minister', 'reform', '–∑–∞–∫–æ–Ω', '–ø–æ–ª–∏—Ç–∏–∫–∞'],
    '–≠–∫–æ–Ω–æ–º–∏–∫–∞': ['economy', 'finance', 'bank', '—Ä—ã–Ω–æ–∫', '—ç–∫–æ–Ω–æ–º–∏–∫–∞'],
    '–°–∫–∞–Ω–¥–∞–ª': ['scandal', 'investigation', '—Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '—Å–∫–∞–Ω–¥–∞–ª', 'corruption'],
    '–í–æ–π–Ω–∞': ['war', 'attack', 'army', '–≤–æ–π–Ω–∞', '—É–¥–∞—Ä', '–∞—Ä–º–∏—è'],
    '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': ['tech', 'startup', 'ai', 'robot', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', 'ai'],
    '–°–ø–æ—Ä—Ç': ['sport', 'match', 'game', '—á–µ–º–ø–∏–æ–Ω–∞—Ç', '—Å–ø–æ—Ä—Ç'],
}
EMOJI_TOPICS = {
    '–°–®–ê': 'üá∫üá∏', '–í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è': 'üá¨üáß', '–ò–∑—Ä–∞–∏–ª—å': 'üáÆüá±', '–†–æ—Å—Å–∏—è': 'üá∑üá∫',
    '–ö–∏—Ç–∞–π': 'üá®üá≥', '–Ø–ø–æ–Ω–∏—è': 'üáØüáµ', '–ò–Ω–¥–∏—è': 'üáÆüá≥', '–ì–µ—Ä–º–∞–Ω–∏—è': 'üá©üá™', '–§—Ä–∞–Ω—Ü–∏—è': 'üá´üá∑', '–ë–ª–∏–∂–Ω–∏–π –í–æ—Å—Ç–æ–∫': 'üåç',
    '–ú—É–∑—ã–∫–∞': 'üéµ', '–ü–æ–ª–∏—Ç–∏–∫–∞': 'üèõÔ∏è', '–≠–∫–æ–Ω–æ–º–∏–∫–∞': 'üí∞', '–°–∫–∞–Ω–¥–∞–ª': 'üí•', '–í–æ–π–Ω–∞': 'üí£', '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': 'ü§ñ', '–°–ø–æ—Ä—Ç': 'üèÖ'
}

def extract_tags(text):
    tags, emojis = [], []
    text_l = text.lower()
    # –°—Ç—Ä–∞–Ω–∞
    for topic, keys in COUNTRY_TOPICS.items():
        if any(k in text_l for k in keys):
            tags.append(f"#{topic.replace(' ', '')}")
            emojis.append(EMOJI_TOPICS.get(topic, 'üåç'))
    # –¢–∏–ø —Å–æ–±—ã—Ç–∏—è
    for topic, keys in EVENT_TOPICS.items():
        if any(k in text_l for k in keys):
            tags.append(f"#{topic}")
            emojis.append(EMOJI_TOPICS.get(topic, '‚ú®'))
    # –ë–∞–∑–æ–≤—ã–µ —Ç–µ–≥–∏
    tags.append("#–ù–û–í–ê_–≥–ª–∞–≤–Ω–æ–µ")
    if not emojis:
        emojis.append("üåè")
    return tags, emojis

def google_translate(text, api_key):
    url = "https://translation.googleapis.com/language/translate/v2"
    params = {"q": text, "target": "ru", "format": "text", "key": api_key}
    response = requests.post(url, data=params)
    if response.status_code == 200:
        res = response.json()
        return res["data"]["translations"][0]["translatedText"]
    return None

def yandex_translate(text, api_key, folder_id):
    url = "https://translate.api.cloud.yandex.net/translate/v2/translate"
    headers = {"Content-Type": "application/json", "Authorization": f"Api-Key {api_key}"}
    body = {"targetLanguageCode": "ru", "texts": [text], "folderId": folder_id}
    response = requests.post(url, headers=headers, json=body)
    if response.status_code == 200:
        res = response.json()
        return res["translations"][0]["text"]
    return None

def summarize_and_translate(title, summary, link):
    prompt = (
        f"–í–æ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º:\n\n"
        f"Title: {title}\n"
        f"Summary: {summary}\n\n"
        f"1. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –ø–æ—Å—Ç –¥–ª—è Telegram –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ: –∫–æ—Ä–æ—Ç–∫–æ, –ø–æ–Ω—è—Ç–Ω–æ, –∂–∏–≤–æ, 2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n"
        f"2. –ü–æ—Å—Ç –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–∞–ª—å–∫–æ–π –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ, –∞ —á–∏—Ç–∞—Ç—å—Å—è –∫–∞–∫ –Ω–∞—Å—Ç–æ—è—â–∞—è –Ω–æ–≤–æ—Å—Ç—å –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö.\n"
        f"3. –ù–µ –ø–∏—à–∏ '–∑–∞–≥–æ–ª–æ–≤–æ–∫' –∏ 'summary', –ø—Ä–æ—Å—Ç–æ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –ø–æ—Å—Ç.\n"
        f"4. –ù–µ –¥–æ–±–∞–≤–ª—è–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ —Ç–µ–∫—Å—Ç, —Ç–æ–ª—å–∫–æ –≤ —Å—Å—ã–ª–∫–µ –≤ –∫–æ–Ω—Ü–µ.\n"
        f"5. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –±–æ–ª—å—à–µ 350 –∑–Ω–∞–∫–æ–≤.\n"
        f"6. –ò—Ç–æ–≥: —Ç–æ–ª—å–∫–æ —Å–∞–º —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è Telegram, –º–∞–∫—Å–∏–º—É–º –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏ –∏ —è—Å–Ω–æ—Å—Ç–∏."
    )
    try:
        client = openai.OpenAI(api_key=OPENAI_KEY)
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "–¢—ã –æ–ø—ã—Ç–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö Telegram-–∫–∞–Ω–∞–ª–æ–≤, –ø–∏—à–∏ –∂–∏–≤–æ –∏ –ø–æ-—Ä—É—Å—Å–∫–∏."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=256,
            temperature=0.6,
        )
        text = response.choices[0].message.content.strip()
    except Exception:
        text = None
    if not text:
        try:
            text_to_translate = f"{title}\n\n{summary}" if summary else title
            text = google_translate(text_to_translate, GOOGLE_KEY)
        except Exception:
            text = None
    if not text:
        try:
            text_to_translate = f"{title}\n\n{summary}" if summary else title
            text = yandex_translate(text_to_translate, YANDEX_KEY, YANDEX_FOLDER_ID)
        except Exception:
            text = None
    if not text:
        text = f"{title}\n\n{summary}"

    tags, emojis = extract_tags(f"{title} {summary}")
    hashtags = ' '.join(tags)
    emojis = ' '.join(emojis)
    return f"{emojis}\n{text}\n\n{hashtags}\n"

def extract_image(entry):
    # –∏—â–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É –≤ rss
    if hasattr(entry, 'media_content'):
        for m in entry.media_content:
            if 'url' in m:
                return m['url']
    if hasattr(entry, 'links'):
        for link in entry.links:
            if link.type and 'image' in link.type:
                return link.href
    if hasattr(entry, 'image') and entry.image and 'href' in entry.image:
        return entry.image['href']
    return None

def post_news(app, channel_id, max_news=3, filter_tag=None):
    global posted_links
    count = 0
    for rss_url in RSS_FEEDS:
        feed = feedparser.parse(rss_url)
        if feed.entries:
            for entry in feed.entries:
                if entry.link in posted_links:
                    continue
                title = entry.title
                summary = entry.summary if hasattr(entry, "summary") else ""
                link = entry.link
                tags, _ = extract_tags(f"{title} {summary}")
                if filter_tag and filter_tag not in tags:
                    continue
                summary_ru = summarize_and_translate(title, summary, link)
                news_text = f"{summary_ru}\n{link}"

                post_id = short_id(entry.link)
                image_url = extract_image(entry)

                # –ö—Ä–∞—Å–∏–≤–∞—è –∫–Ω–æ–ø–∫–∞ "–ü–æ–¥—Ä–æ–±–Ω–µ–µ" + –ø–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ —Ç–µ–º—É
                from pyrogram.types import InlineKeyboardButton
                main_tag = tags[0] if tags else "–ú–∏—Ä"
                buttons = [
                    [InlineKeyboardButton("üì∞ –ü–æ–¥—Ä–æ–±–Ω–µ–µ", url=link)],
                    [InlineKeyboardButton("üîî –ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ —Ç–µ–º—É", callback_data=f"subscribe:{main_tag}")]
                    
                ]
                reply_markup = get_reactions_keyboard(post_id, extra_buttons=buttons)

                if image_url:
                    app.send_photo(
                        channel_id,
                        photo=image_url,
                        caption=news_text,
                        reply_markup=reply_markup
                    )
                else:
                    app.send_message(
                        channel_id,
                        news_text,
                        reply_markup=reply_markup
                    )

                posted_links.add(link)
                count += 1
                if count >= max_news:
                    save_posted_links(posted_links)
                    print(f"–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ {count} –Ω–æ–≤–æ—Å—Ç–µ–π.")
                    return
    save_posted_links(posted_links)
    if count == 0:
        print("–°–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

def run_autoposting(app, channel_id):
    print("–ê–≤—Ç–æ–ø–æ—Å—Ç–∏–Ω–≥ —Å —Ä–µ–∞–∫—Ü–∏—è–º–∏ –∏ –∞–ø–≥—Ä–µ–π–¥–∞–º–∏ –∑–∞–ø—É—â–µ–Ω!")
    post_news(app, channel_id, max_news=3)
    schedule.every(10).minutes.do(post_news, app, channel_id, max_news=3)
    while True:
        schedule.run_pending()
        time.sleep(1)

def get_stats():
    tag_counts = {}
    for rss_url in RSS_FEEDS:
        feed = feedparser.parse(rss_url)
        if feed.entries:
            for entry in feed.entries:
                title = entry.title
                summary = entry.summary if hasattr(entry, "summary") else ""
                tags, _ = extract_tags(f"{title} {summary}")
                for t in tags:
                    tag_counts[t] = tag_counts.get(t, 0) + 1
    stats = "\n".join([f"{k}: {v}" for k, v in sorted(tag_counts.items(), key=lambda x: -x[1])])
    return f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–º–∞–º:\n{stats or '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö.'}"
# –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–µ–≥—É
def get_news_by_tag(tag, posted_links, max_news=3):
    results = []
    tag_l = tag.lower()
    for rss_url in RSS_FEEDS:
        feed = feedparser.parse(rss_url)
        if feed.entries:
            for entry in feed.entries:
                if entry.link in posted_links:
                    continue
                title = entry.title
                summary = entry.summary if hasattr(entry, "summary") else ""
                tags, _ = extract_tags(f"{title} {summary}")
                if any(tag_l in t.lower() for t in tags):
                    summary_ru = summarize_and_translate(title, summary, entry.link)
                    news_text = f"{summary_ru}\n\n{entry.link}" if entry.link not in summary_ru else summary_ru
                    results.append((news_text, entry.link))
                    if len(results) >= max_news:
                        return results
    return results

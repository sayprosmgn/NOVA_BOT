import schedule
import time
import feedparser
import pickle
import os
import openai
import requests
from config import OPENAI_KEY, GOOGLE_KEY, YANDEX_KEY, YANDEX_FOLDER_ID
from handlers.reactions import get_reactions_keyboard
import hashlib

POSTED_LINKS_FILE = "posted_links.pkl"

def short_id(link):
    return hashlib.md5(link.encode()).hexdigest()[:16]

def load_posted_links():
    if os.path.exists(POSTED_LINKS_FILE):
        with open(POSTED_LINKS_FILE, "rb") as f:
            return pickle.load(f)
    return set()

def save_posted_links(posted_links):
    with open(POSTED_LINKS_FILE, "wb") as f:
        pickle.dump(posted_links, f)

posted_links = load_posted_links()

# RSS-Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ (Ğ¾ÑÑ‚Ğ°Ğ²ÑŒ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸)
RSS_FEEDS = [
    "http://feeds.bbci.co.uk/news/world/rss.xml",
    "https://feeds.nbcnews.com/nbcnews/public/news",
    # ... Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ...
    "https://www.aljazeera.com/xml/rss/all.xml",
    "https://www.reutersagency.com/feed/?best-topics=world&post_type=best",
    "https://apnews.com/rss",
]

COUNTRY_TOPICS = {
    'Ğ¡Ğ¨Ğ':      ['usa', 'trump', 'biden', 'white house', 'us ', 'washington', 'america', 'cnn', 'nbc', 'new york'],
    'Ğ’ĞµĞ»Ğ¸ĞºĞ¾Ğ±Ñ€Ğ¸Ñ‚Ğ°Ğ½Ğ¸Ñ': ['uk', 'britain', 'london', 'bbc', 'sky news', 'guardian'],
    'Ğ˜Ğ·Ñ€Ğ°Ğ¸Ğ»ÑŒ':  ['israel', 'jerusalem', 'tel aviv', 'idf', 'gaza', 'hamas', 'jpost', 'haaretz'],
    'Ğ Ğ¾ÑÑĞ¸Ñ':   ['russia', 'moscow', 'kremlin', 'putin', 'rbc', 'lenta', 'tass', 'meduza'],
    'ĞšĞ¸Ñ‚Ğ°Ğ¹':    ['china', 'beijing', 'xi jinping', 'scmp'],
    'Ğ¯Ğ¿Ğ¾Ğ½Ğ¸Ñ':   ['japan', 'tokyo', 'kyodo'],
    'Ğ˜Ğ½Ğ´Ğ¸Ñ':    ['india', 'delhi', 'hindustan'],
    'Ğ“ĞµÑ€Ğ¼Ğ°Ğ½Ğ¸Ñ': ['germany', 'berlin', 'dw', 'spiegel'],
    'Ğ¤Ñ€Ğ°Ğ½Ñ†Ğ¸Ñ':  ['france', 'paris', 'france 24'],
    'Ğ‘Ğ»Ğ¸Ğ¶Ğ½Ğ¸Ğ¹ Ğ’Ğ¾ÑÑ‚Ğ¾Ğº': ['iran', 'iraq', 'syria', 'qatar', 'arab', 'al arabiya', 'aljazeera'],
}
EVENT_TOPICS = {
    'ĞœÑƒĞ·Ñ‹ĞºĞ°': ['music', 'concert', 'song', 'album', 'duet', 'popstar', 'Ğ¼ÑƒĞ·Ñ‹ĞºĞ°', 'Ğ¿ĞµÑĞ½Ñ', 'ĞºĞ¾Ğ½Ñ†ĞµÑ€Ñ‚'],
    'ĞŸĞ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°': ['politics', 'president', 'minister', 'reform', 'Ğ·Ğ°ĞºĞ¾Ğ½', 'Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°'],
    'Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°': ['economy', 'finance', 'bank', 'Ñ€Ñ‹Ğ½Ğ¾Ğº', 'ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°'],
    'Ğ¡ĞºĞ°Ğ½Ğ´Ğ°Ğ»': ['scandal', 'investigation', 'Ñ€Ğ°ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'ÑĞºĞ°Ğ½Ğ´Ğ°Ğ»', 'corruption'],
    'Ğ’Ğ¾Ğ¹Ğ½Ğ°': ['war', 'attack', 'army', 'Ğ²Ğ¾Ğ¹Ğ½Ğ°', 'ÑƒĞ´Ğ°Ñ€', 'Ğ°Ñ€Ğ¼Ğ¸Ñ'],
    'Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸': ['tech', 'startup', 'ai', 'robot', 'Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸', 'ai'],
    'Ğ¡Ğ¿Ğ¾Ñ€Ñ‚': ['sport', 'match', 'game', 'Ñ‡ĞµĞ¼Ğ¿Ğ¸Ğ¾Ğ½Ğ°Ñ‚', 'ÑĞ¿Ğ¾Ñ€Ñ‚'],
}
EMOJI_TOPICS = {
    'Ğ¡Ğ¨Ğ': 'ğŸ‡ºğŸ‡¸', 'Ğ’ĞµĞ»Ğ¸ĞºĞ¾Ğ±Ñ€Ğ¸Ñ‚Ğ°Ğ½Ğ¸Ñ': 'ğŸ‡¬ğŸ‡§', 'Ğ˜Ğ·Ñ€Ğ°Ğ¸Ğ»ÑŒ': 'ğŸ‡®ğŸ‡±', 'Ğ Ğ¾ÑÑĞ¸Ñ': 'ğŸ‡·ğŸ‡º',
    'ĞšĞ¸Ñ‚Ğ°Ğ¹': 'ğŸ‡¨ğŸ‡³', 'Ğ¯Ğ¿Ğ¾Ğ½Ğ¸Ñ': 'ğŸ‡¯ğŸ‡µ', 'Ğ˜Ğ½Ğ´Ğ¸Ñ': 'ğŸ‡®ğŸ‡³', 'Ğ“ĞµÑ€Ğ¼Ğ°Ğ½Ğ¸Ñ': 'ğŸ‡©ğŸ‡ª', 'Ğ¤Ñ€Ğ°Ğ½Ñ†Ğ¸Ñ': 'ğŸ‡«ğŸ‡·', 'Ğ‘Ğ»Ğ¸Ğ¶Ğ½Ğ¸Ğ¹ Ğ’Ğ¾ÑÑ‚Ğ¾Ğº': 'ğŸŒ',
    'ĞœÑƒĞ·Ñ‹ĞºĞ°': 'ğŸµ', 'ĞŸĞ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°': 'ğŸ›ï¸', 'Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°': 'ğŸ’°', 'Ğ¡ĞºĞ°Ğ½Ğ´Ğ°Ğ»': 'ğŸ’¥', 'Ğ’Ğ¾Ğ¹Ğ½Ğ°': 'ğŸ’£', 'Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸': 'ğŸ¤–', 'Ğ¡Ğ¿Ğ¾Ñ€Ñ‚': 'ğŸ…'
}

def extract_tags(text):
    tags, emojis = [], []
    text_l = text.lower()
    # Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ°
    for topic, keys in COUNTRY_TOPICS.items():
        if any(k in text_l for k in keys):
            tags.append(f"#{topic.replace(' ', '')}")
            emojis.append(EMOJI_TOPICS.get(topic, 'ğŸŒ'))
    # Ğ¢Ğ¸Ğ¿ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ
    for topic, keys in EVENT_TOPICS.items():
        if any(k in text_l for k in keys):
            tags.append(f"#{topic}")
            emojis.append(EMOJI_TOPICS.get(topic, 'âœ¨'))
    # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµĞ³Ğ¸
    tags.append("#ĞĞĞ’Ğ_Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğµ")
    if not emojis:
        emojis.append("ğŸŒ")
    return tags, emojis

def google_translate(text, api_key):
    url = "https://translation.googleapis.com/language/translate/v2"
    params = {"q": text, "target": "ru", "format": "text", "key": api_key}
    response = requests.post(url, data=params)
    if response.status_code == 200:
        res = response.json()
        return res["data"]["translations"][0]["translatedText"]
    return None

def yandex_translate(text, api_key, folder_id):
    url = "https://translate.api.cloud.yandex.net/translate/v2/translate"
    headers = {"Content-Type": "application/json", "Authorization": f"Api-Key {api_key}"}
    body = {"targetLanguageCode": "ru", "texts": [text], "folderId": folder_id}
    response = requests.post(url, headers=headers, json=body)
    if response.status_code == 200:
        res = response.json()
        return res["translations"][0]["text"]
    return None

def summarize_and_translate(title, summary, link):
    prompt = (
        f"Ğ’Ğ¾Ñ‚ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ¸ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼:\n\n"
        f"Title: {title}\n"
        f"Summary: {summary}\n\n"
        f"1. Ğ¡Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞ¹ Ğ¿Ğ¾ÑÑ‚ Ğ´Ğ»Ñ Telegram Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ: ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾, Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾, Ğ¶Ğ¸Ğ²Ğ¾, 2â€“3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ.\n"
        f"2. ĞŸĞ¾ÑÑ‚ Ğ½Ğµ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ ĞºĞ°Ğ»ÑŒĞºĞ¾Ğ¹ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾, Ğ° Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒÑÑ ĞºĞ°Ğº Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰Ğ°Ñ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€ÑƒÑÑĞºĞ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ….\n"
        f"3. ĞĞµ Ğ¿Ğ¸ÑˆĞ¸ 'Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº' Ğ¸ 'summary', Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚.\n"
        f"4. ĞĞµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ² Ñ‚ĞµĞºÑÑ‚, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² ÑÑÑ‹Ğ»ĞºĞµ Ğ² ĞºĞ¾Ğ½Ñ†Ğµ.\n"
        f"5. ĞĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ 350 Ğ·Ğ½Ğ°ĞºĞ¾Ğ².\n"
        f"6. Ğ˜Ñ‚Ğ¾Ğ³: Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ°Ğ¼ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Telegram, Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ¸ĞºĞ¸ Ğ¸ ÑÑĞ½Ğ¾ÑÑ‚Ğ¸."
    )
    try:
        client = openai.OpenAI(api_key=OPENAI_KEY)
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Ğ¢Ñ‹ Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Telegram-ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ², Ğ¿Ğ¸ÑˆĞ¸ Ğ¶Ğ¸Ğ²Ğ¾ Ğ¸ Ğ¿Ğ¾-Ñ€ÑƒÑÑĞºĞ¸."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=256,
            temperature=0.6,
        )
        text = response.choices[0].message.content.strip()
    except Exception:
        text = None
    if not text:
        try:
            text_to_translate = f"{title}\n\n{summary}" if summary else title
            text = google_translate(text_to_translate, GOOGLE_KEY)
        except Exception:
            text = None
    if not text:
        try:
            text_to_translate = f"{title}\n\n{summary}" if summary else title
            text = yandex_translate(text_to_translate, YANDEX_KEY, YANDEX_FOLDER_ID)
        except Exception:
            text = None
    if not text:
        text = f"{title}\n\n{summary}"

    tags, emojis = extract_tags(f"{title} {summary}")
    hashtags = ' '.join(tags)
    emojis = ' '.join(emojis)
    return f"{emojis}\n{text}\n\n{hashtags}\n"

def extract_image(entry):
    # Ğ¸Ñ‰ĞµÑ‚ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºÑƒ Ğ² rss
    if hasattr(entry, 'media_content'):
        for m in entry.media_content:
            if 'url' in m:
                return m['url']
    if hasattr(entry, 'links'):
        for link in entry.links:
            if link.type and 'image' in link.type:
                return link.href
    if hasattr(entry, 'image') and entry.image and 'href' in entry.image:
        return entry.image['href']
    return None

def post_news(app, channel_id, max_news=3, filter_tag=None):
    global posted_links
    count = 0
    for rss_url in RSS_FEEDS:
        feed = feedparser.parse(rss_url)
        if feed.entries:
            for entry in feed.entries:
                if entry.link in posted_links:
                    continue
                title = entry.title
                summary = entry.summary if hasattr(entry, "summary") else ""
                link = entry.link
                tags, _ = extract_tags(f"{title} {summary}")
                if filter_tag and filter_tag not in tags:
                    continue
                summary_ru = summarize_and_translate(title, summary, link)
                news_text = f"{summary_ru}\n{link}"

                post_id = short_id(entry.link)
                image_url = extract_image(entry)

                # ĞšÑ€Ğ°ÑĞ¸Ğ²Ğ°Ñ ĞºĞ½Ğ¾Ğ¿ĞºĞ° "ĞŸĞ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ" + Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞºĞ° Ğ½Ğ° Ñ‚ĞµĞ¼Ñƒ
                from pyrogram.types import InlineKeyboardButton
                main_tag = tags[0] if tags else "ĞœĞ¸Ñ€"
                buttons = [
                    [InlineKeyboardButton("ğŸ“° ĞŸĞ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ", url=link)],
                    [InlineKeyboardButton("ğŸ”” ĞŸĞ¾Ğ´Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ñ‚ĞµĞ¼Ñƒ", callback_data=f"subscribe:{main_tag}")]
                    
                ]
                reply_markup = get_reactions_keyboard(post_id, extra_buttons=buttons)

                if image_url:
                    app.send_photo(
                        channel_id,
                        photo=image_url,
                        caption=news_text,
                        reply_markup=reply_markup
                    )
                else:
                    app.send_message(
                        channel_id,
                        news_text,
                        reply_markup=reply_markup
                    )

                posted_links.add(link)
                count += 1
                if count >= max_news:
                    save_posted_links(posted_links)
                    print(f"ĞĞ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ¾ {count} Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹.")
                    return
    save_posted_links(posted_links)
    if count == 0:
        print("Ğ¡Ğ²ĞµĞ¶Ğ¸Ñ… Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾.")

def run_autoposting(app, channel_id):
    print("ĞĞ²Ñ‚Ğ¾Ğ¿Ğ¾ÑÑ‚Ğ¸Ğ½Ğ³ Ñ Ñ€ĞµĞ°ĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ°Ğ¿Ğ³Ñ€ĞµĞ¹Ğ´Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½!")
    post_news(app, channel_id, max_news=3)
    schedule.every(10).minutes.do(post_news, app, channel_id, max_news=3)
    while True:
        schedule.run_pending()
        time.sleep(1)

def get_stats():
    tag_counts = {}
    for rss_url in RSS_FEEDS:
        feed = feedparser.parse(rss_url)
        if feed.entries:
            for entry in feed.entries:
                title = entry.title
                summary = entry.summary if hasattr(entry, "summary") else ""
                tags, _ = extract_tags(f"{title} {summary}")
                for t in tags:
                    tag_counts[t] = tag_counts.get(t, 0) + 1
    stats = "\n".join([f"{k}: {v}" for k, v in sorted(tag_counts.items(), key=lambda x: -x[1])])
    return f"Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿Ğ¾ Ñ‚ĞµĞ¼Ğ°Ğ¼:\n{stats or 'ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}"
# ĞŸĞ¾Ğ¸ÑĞº Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾ Ñ‚ĞµĞ³Ñƒ
def get_news_by_tag(tag, posted_links, max_news=3):
    results = []
    tag_l = tag.lower()
    for rss_url in RSS_FEEDS:
        feed = feedparser.parse(rss_url)
        if feed.entries:
            for entry in feed.entries:
                if entry.link in posted_links:
                    continue
                title = entry.title
                summary = entry.summary if hasattr(entry, "summary") else ""
                tags, _ = extract_tags(f"{title} {summary}")
                if any(tag_l in t.lower() for t in tags):
                    summary_ru = summarize_and_translate(title, summary, entry.link)
                    news_text = f"{summary_ru}\n\n{entry.link}" if entry.link not in summary_ru else summary_ru
                    results.append((news_text, entry.link))
                    if len(results) >= max_news:
                        return results
    return results
